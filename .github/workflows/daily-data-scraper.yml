name: Daily Data Scraper

on:
  # Run daily at 6 AM UTC
  schedule:
    - cron: '0 6 * * *'

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      scrape_mode:
        description: 'Scraping mode'
        required: true
        default: 'full'
        type: choice
        options:
          - full
          - quick
          - data-only
      enable_scraping:
        description: 'Enable web scraping (slower but gets fresh data)'
        required: false
        default: false
        type: boolean

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install supabase python-dotenv requests beautifulsoup4 gspread google-auth

      - name: Create .env file
        run: |
          cat << EOF > scripts/.env
          SUPABASE_URL=${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY=${{ secrets.SUPABASE_KEY }}
          SUPABASE_ROLE_KEY=${{ secrets.SUPABASE_ROLE_KEY }}
          SCRAPER_USERNAME=${{ secrets.SCRAPER_USERNAME }}
          SCRAPER_PASSWORD=${{ secrets.SCRAPER_PASSWORD }}
          EOF

      - name: Create Google credentials file
        run: |
          echo '${{ secrets.GOOGLE_CREDENTIALS_JSON }}' > google-credentials.json

      - name: Run data scraper (Scheduled - Full Pipeline)
        if: github.event_name == 'schedule'
        working-directory: ./scripts
        run: |
          echo "Running scheduled daily scraping..."
          python main.py --full --scrape

      - name: Run data scraper (Manual - Full)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.scrape_mode == 'full'
        working-directory: ./scripts
        run: |
          if [ "${{ github.event.inputs.enable_scraping }}" == "true" ]; then
            echo "Running full pipeline with web scraping..."
            python main.py --full --scrape
          else
            echo "Running full pipeline without web scraping..."
            python main.py --full
          fi

      - name: Run data scraper (Manual - Quick)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.scrape_mode == 'quick'
        working-directory: ./scripts
        run: |
          echo "Running quick update (management + analytics only)..."
          python main.py --quick

      - name: Run data scraper (Manual - Data Only)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.scrape_mode == 'data-only'
        working-directory: ./scripts
        run: |
          if [ "${{ github.event.inputs.enable_scraping }}" == "true" ]; then
            echo "Running data processing with web scraping..."
            python main.py --data --scrape
          else
            echo "Running data processing without web scraping..."
            python main.py --data
          fi

      - name: Sync attendance from Google Sheets
        working-directory: ./scripts
        run: |
          echo "üìä Syncing attendance data from Google Sheets..."
          python main.py --attendance || echo "‚ö†Ô∏è Attendance sync failed (non-fatal)"

      - name: Commit and push updated data
        if: success()
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add public/student_grades.json
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update student grades data

          ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

          Co-Authored-By: Claude <noreply@anthropic.com>"
            git push
            echo "‚úÖ Successfully updated student_grades.json"
          fi

      - name: Check for student status changes and send notifications
        if: success()
        run: |
          echo "üîç Checking for student status changes..."
          RESPONSE=$(curl -s -X POST ${{ secrets.APP_URL }}/api/notifications/monitor-status \
            -H "Content-Type: application/json" \
            -w "\nHTTP_STATUS:%{http_code}")

          HTTP_STATUS=$(echo "$RESPONSE" | grep "HTTP_STATUS" | cut -d: -f2)
          BODY=$(echo "$RESPONSE" | sed '/HTTP_STATUS/d')

          if [ "$HTTP_STATUS" -eq 200 ]; then
            echo "‚úÖ Status monitoring completed successfully"
            echo "$BODY" | jq -r '.changes_detected as $count | if $count > 0 then "üìß Sent notifications for \($count) student(s) with status changes" else "‚ÑπÔ∏è No status changes detected" end'
          else
            echo "‚ö†Ô∏è Status monitoring failed with HTTP $HTTP_STATUS"
            echo "$BODY"
            # Don't fail the entire workflow, just log the error
          fi

      - name: Clean up sensitive files
        if: always()
        run: |
          rm -f scripts/.env
          rm -f google-credentials.json

      - name: Upload logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: |
            scripts/*.log
            scripts/logs/
          retention-days: 7

      - name: Notify on failure
        if: failure()
        run: |
          echo "::error::Data scraping pipeline failed. Check the logs for details."
          echo "Run number: ${{ github.run_number }}"
          echo "Run ID: ${{ github.run_id }}"
